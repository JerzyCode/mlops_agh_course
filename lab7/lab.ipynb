{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33097f9b",
   "metadata": {},
   "source": [
    "# Lab 07 MLops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78c8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 21 16:12:15 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.274.02             Driver Version: 535.274.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:5B:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b196e7d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac18d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa96556",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef76194",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67301412",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44216997",
   "metadata": {},
   "source": [
    "### Sample input - used for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6f24d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: Hello World1!\n",
      "TOKENS: ['<s>', 'hello', 'world', '##1', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "IDS:[0, 7596, 2092, 2491, 1003, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "TEXT: Java is better programming language than python - change my mind.\n",
      "TOKENS: ['<s>', 'java', 'is', 'better', 'programming', 'language', 'than', 'python', '-', 'change', 'my', 'mind', '.', '</s>']\n",
      "IDS:[0, 9266, 2007, 2492, 4734, 2657, 2088, 18754, 1015, 2693, 2030, 2572, 1016, 2]\n",
      "\n",
      "TEXT: Transformers are powerful models for NLP.\n",
      "TOKENS: ['<s>', 'transformers', 'are', 'powerful', 'models', 'for', 'nl', '##p', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "IDS:[0, 19085, 2028, 3932, 4279, 2009, 17957, 2365, 1016, 2, 1, 1, 1, 1]\n",
      "\n",
      "TEXT: Batch processing allows multiple sentences to be encoded together.\n",
      "TOKENS: ['<s>', 'batch', 'processing', 'allows', 'multiple', 'sentences', 'to', 'be', 'encoded', 'together', '.', '</s>', '<pad>', '<pad>']\n",
      "IDS:[0, 14112, 6368, 4477, 3678, 11750, 2004, 2026, 12363, 2366, 1016, 2, 1, 1]\n",
      "\n",
      "TEXT: This is another example input.\n",
      "TOKENS: ['<s>', 'this', 'is', 'another', 'example', 'input', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "IDS:[0, 2027, 2007, 2182, 2746, 7957, 1016, 2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Hello World1!\",\n",
    "    \"Java is better programming language than python - change my mind.\",\n",
    "    \"Transformers are powerful models for NLP.\",\n",
    "    \"Batch processing allows multiple sentences to be encoded together.\",\n",
    "    \"This is another example input.\"\n",
    "]\n",
    "\n",
    "encoded_inputs = tokenizer(\n",
    "    texts,\n",
    "    padding=True,          \n",
    "    truncation=True,      \n",
    "    return_tensors=\"pt\"    \n",
    ")\n",
    "\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "\n",
    "for text, ids in zip(texts, input_ids):\n",
    "    token_list = tokenizer.convert_ids_to_tokens(ids.tolist())\n",
    "    print(f\"\\nTEXT: {text}\\nTOKENS: {token_list}\\nIDS:{ids.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6aa91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs_cpu = {k: v.to(\"cpu\") for k, v in encoded_inputs.items()}\n",
    "encoded_inputs_gpu = {k: v.to(\"cuda\") for k, v in encoded_inputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969eba77",
   "metadata": {},
   "source": [
    "### Measuring inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85232916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, encoded_inputs, runs=200):\n",
    "    _ = model(**encoded_inputs) # warmup\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for _ in range(runs):\n",
    "        outputs = model(**encoded_inputs)\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    return total_time / runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f2ebe",
   "metadata": {},
   "source": [
    "#### Plain Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb305c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Plain Torch CPU: 0.2303 s\n",
      "Time Plain Torch GPU: 0.0081 s\n"
     ]
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "plain_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "print(f\"Time Plain Torch CPU: {plain_cpu:.4f} s\")\n",
    "\n",
    "model.to(device=\"cuda\")\n",
    "plain_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "print(f\"Time Plain Torch GPU: {plain_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54d802",
   "metadata": {},
   "source": [
    "#### Eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8b8b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Eval Torch CPU: 0.2332 s\n",
      "Time Eval Torch GPU: 0.0080 s\n"
     ]
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "model.eval()\n",
    "eval_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "print(f\"Time Eval Torch CPU: {eval_cpu:.4f} s\")\n",
    "\n",
    "model.to(device=\"cuda\")\n",
    "model.eval()\n",
    "eval_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "print(f\"Time Eval Torch GPU: {eval_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c9216",
   "metadata": {},
   "source": [
    "#### Eval mode and no grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1130ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Eval and no Grad Torch CPU: 0.2250 s\n",
      "Time Eval and no Grad Torch GPU: 0.0063 s\n"
     ]
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_no_grad_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "    print(f\"Time Eval and no Grad Torch CPU: {eval_no_grad_cpu:.4f} s\")\n",
    "\n",
    "model.to(device=\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_no_grad_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "    print(f\"Time Eval and no Grad Torch GPU: {eval_no_grad_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113b7ba",
   "metadata": {},
   "source": [
    "#### Full inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73277c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Inference Mode CPU: 0.2214 s\n",
      "Time Inference Mode GPU: 0.0056 s\n"
     ]
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "with torch.inference_mode():\n",
    "    eval_inference_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "    print(f\"Time Inference Mode CPU: {eval_inference_cpu:.4f} s\")\n",
    "\n",
    "model.to(device=\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    eval_inference_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "    print(f\"Time Inference Mode GPU: {eval_inference_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293a553",
   "metadata": {},
   "source": [
    "### Result of experiment\n",
    "\n",
    "| Method                     | CPU Time (s) | GPU Time (s) |\n",
    "|----------------------------|-------------|-------------|\n",
    "| Plain Torch                | 0.2303      | 0.0081      |\n",
    "| Eval Torch                 | 0.2332      | 0.0080      |\n",
    "| Eval Torch (no Grad)       | 0.2250      | 0.0063      |\n",
    "| Inference Mode             | 0.2214      | 0.0056      |\n",
    "\n",
    "\n",
    "The fastest is the inference mode. Interestingly, the Eval Torch on CPU is slower than plan torch on CPU. For the GPU Difference is small, but eval is faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8a2c2",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Compiling only for GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791e0af",
   "metadata": {},
   "source": [
    "### 1. Compiling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d436a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device=\"cuda\")\n",
    "compiled_model_gpu = torch.compile(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082baaa5",
   "metadata": {},
   "source": [
    "### 2. Measuring time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c331521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Compiled GPU: 0.0048 s\n"
     ]
    }
   ],
   "source": [
    "eval_compiled_gpu = measure_inference_time(compiled_model_gpu, encoded_inputs_gpu)\n",
    "print(f\"Time Compiled GPU: {eval_compiled_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3e1aa",
   "metadata": {},
   "source": [
    "### 3. Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6875\n"
     ]
    }
   ],
   "source": [
    "speed_up = 0.0081 / 0.0048\n",
    "print(speed_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd7f74",
   "metadata": {},
   "source": [
    "Compiled model is faster than all of previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef3ecc",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5166ab",
   "metadata": {},
   "source": [
    "### 1. Ensuring model on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2909e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca867020",
   "metadata": {},
   "source": [
    "### 2. Quantize model\n",
    "\n",
    "### 3. Save model to variable, veryfing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e1ad2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model:\n",
      "MPNetModel(\n",
      "  (embeddings): MPNetEmbeddings(\n",
      "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): MPNetEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MPNetLayer(\n",
      "        (attention): MPNetAttention(\n",
      "          (attn): MPNetSelfAttention(\n",
      "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (intermediate): MPNetIntermediate(\n",
      "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MPNetOutput(\n",
      "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (relative_attention_bias): Embedding(32, 12)\n",
      "  )\n",
      "  (pooler): MPNetPooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.ao.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "model_quantized = torch.ao.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {nn.Linear}, \n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"Quantized model:\")\n",
    "print(model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffbfdf",
   "metadata": {},
   "source": [
    "### 4. Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd6d2366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 417.73 MB\n",
      "Quantized model size: 173.10 MB\n",
      "Compression ratio: 2.41×\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "torch.save(model.state_dict(), \"model_original.pt\")\n",
    "torch.save(model_quantized.state_dict(), \"model_quantized.pt\")\n",
    "\n",
    "size_original = os.path.getsize(\"model_original.pt\")\n",
    "size_quantized = os.path.getsize(\"model_quantized.pt\")\n",
    "\n",
    "size_original_mb = size_original / (1024 * 1024)\n",
    "size_quantized_mb = size_quantized / (1024 * 1024)\n",
    "\n",
    "print(f\"Original model size: {size_original_mb:.2f} MB\")\n",
    "print(f\"Quantized model size: {size_quantized_mb:.2f} MB\")\n",
    "print(f\"Compression ratio: {size_original_mb / size_quantized_mb:.2f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27026ee0",
   "metadata": {},
   "source": [
    "### 5. Inference comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7ce6cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Quantized CPU: 0.0626 s\n",
      "Time Non Quantized CPU: 0.2359 s\n"
     ]
    }
   ],
   "source": [
    "quantized_cpu = measure_inference_time(model_quantized, encoded_inputs_cpu)\n",
    "print(f\"Time Quantized CPU: {quantized_cpu:.4f} s\")\n",
    "\n",
    "no_quantized_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "print(f\"Time Non Quantized CPU: {no_quantized_cpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0bb2e0",
   "metadata": {},
   "source": [
    "### 6. Comments on comparission\n",
    "\n",
    "Quantized model has size of 173.10MB while original 417.73MB which is over 2.41 more. Furthermore, quantized model is almost 4 times faster than plain model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c359e5",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46a517",
   "metadata": {},
   "source": [
    "### 1. Comparing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb662b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device=\"cuda\")\n",
    "compiled_model_gpu = torch.compile(model) \n",
    "compiled_model_gpu_max_autotune = torch.compile(model, mode=\"max-autotune\") \n",
    "compiled_model_gpu_max_autotune_no_cuda_graphs = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549ddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    _ = compiled_model_gpu(**encoded_inputs_gpu)\n",
    "    _ = compiled_model_gpu_max_autotune(**encoded_inputs_gpu)\n",
    "    _ = compiled_model_gpu_max_autotune_no_cuda_graphs(**encoded_inputs_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017b4e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Compiled GPU (inference_mode): 0.0031 s\n",
      "Time Compiled GPU Max Autotune (inference_mode): 0.0022 s\n",
      "Time Compiled GPU Max Autotune No CUDA Graphs (inference_mode): 0.0032 s\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    eval_compiled_gpu = measure_inference_time(compiled_model_gpu, encoded_inputs_gpu)\n",
    "    print(f\"Time Compiled GPU (inference_mode): {eval_compiled_gpu:.4f} s\")\n",
    "\n",
    "    eval_compiled_gpu_max_autotune = measure_inference_time(compiled_model_gpu_max_autotune, encoded_inputs_gpu)\n",
    "    print(f\"Time Compiled GPU Max Autotune (inference_mode): {eval_compiled_gpu_max_autotune:.4f} s\")\n",
    "\n",
    "    eval_compiled_gpu_max_autotune_no_cuda_graphs = measure_inference_time(\n",
    "        compiled_model_gpu_max_autotune_no_cuda_graphs, encoded_inputs_gpu\n",
    "    )\n",
    "    print(f\"Time Compiled GPU Max Autotune No CUDA Graphs (inference_mode): {eval_compiled_gpu_max_autotune_no_cuda_graphs:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba2d80",
   "metadata": {},
   "source": [
    "### 2. Comment on results\n",
    "\n",
    "| Model                                    | Time |\n",
    "| ---------------------------------------- | ------------- |\n",
    "| Compiled GPU (default)                   | 0.0031 s      |\n",
    "| Compiled GPU Max Autotune                | 0.0022 s      |\n",
    "| Compiled GPU Max Autotune No CUDA Graphs | 0.0032 s      |\n",
    "\n",
    "\n",
    "The fastest was Compiled GPU Max Autotune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77824bef",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a11cb",
   "metadata": {},
   "source": [
    "### 1. Check capability of GPU Tensor Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a093dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (7, 0)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92fbba",
   "metadata": {},
   "source": [
    "### 2. Measruing inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3bbd306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Full Precisionn Mode GPU: 0.0058 s\n",
      "Time HAlf Precisionn Mode GPU: 0.0057 s\n",
      "Time Autocast Precisionn Mode GPU: 0.0070 s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device=\"cuda\")\n",
    "\n",
    "model_half = model.half().to('cuda')\n",
    "model_half.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    full_precision_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "    print(f\"Time Full Precisionn Mode GPU: {full_precision_gpu:.4f} s\")\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    half_precision_gpu = measure_inference_time(model_half, encoded_inputs_gpu)\n",
    "    print(f\"Time HAlf Precisionn Mode GPU: {half_precision_gpu:.4f} s\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        auto_precision_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "        print(f\"Time Autocast Precisionn Mode GPU: {auto_precision_gpu:.4f} s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4edd8",
   "metadata": {},
   "source": [
    "### 3. Time comparision\n",
    "\n",
    "| Type                  | Time [s]   |\n",
    "|-----------------------|------------|\n",
    "| Full Precision (FP32) | 0.0058     |\n",
    "| Half Precision (FP16) | 0.0057     |\n",
    "| Autocast (Mixed FP16) | 0.0070     |\n",
    "\n",
    "\n",
    "I would use Half precision. It is th fastest there and it is supported well for majority of gpus. However, the test samples count may not be enough to test real differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d24ca",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52f9f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def measure_inference_time_onnx(session: ort.InferenceSession, encoded_inputs: dict, runs: int = 100):\n",
    "    _ = session.run(None, encoded_inputs) #warmup\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for _ in range(runs):\n",
    "        outputs = session.run(None, encoded_inputs)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    return total_time / runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d90c2",
   "metadata": {},
   "source": [
    "### 1. Measure online and offline model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d1771",
   "metadata": {},
   "source": [
    "**transforming to onnx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae5eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "model_cpu = model.eval().cpu()\n",
    "\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"non_opt_model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f166e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "sess_options.intra_op_num_threads = 1\n",
    "sess_options.inter_op_num_threads = 2\n",
    "sess_options.enable_cpu_mem_arena = True\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "session = ort.InferenceSession(\"non_opt_model.onnx\", sess_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f77f6c",
   "metadata": {},
   "source": [
    "**comparision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "781dbd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt model loading time: 0.2370 s\n"
     ]
    }
   ],
   "source": [
    "opt_load_start = time.perf_counter()\n",
    "sess_opt_options = ort.SessionOptions()\n",
    "sess_opt_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "sess_opt_options.intra_op_num_threads = 1\n",
    "sess_opt_options.inter_op_num_threads = 2 \n",
    "sess_opt_options.enable_cpu_mem_arena = True\n",
    "\n",
    "ort_session_optimized = ort.InferenceSession(\n",
    "    \"model_optimized.onnx\", \n",
    "    sess_options=sess_opt_options, \n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "opt_load_end = time.perf_counter()\n",
    "opt_load_time = opt_load_end - opt_load_start\n",
    "print(f\"Opt model loading time: {opt_load_time:.4f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15bacdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Opt model loading time: 0.9791 s\n"
     ]
    }
   ],
   "source": [
    "no_opt_load_start = time.perf_counter()\n",
    "sess_no_opt_options = ort.SessionOptions()\n",
    "sess_no_opt_options.intra_op_num_threads = 1\n",
    "sess_no_opt_options.inter_op_num_threads = 2\n",
    "sess_no_opt_options.enable_cpu_mem_arena = True\n",
    "sess_no_opt_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "ort_no_opt_session = ort.InferenceSession(\n",
    "    \"non_opt_model.onnx\", sess_options=sess_no_opt_options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "no_opt_load_end = time.perf_counter()\n",
    "no_opt_load_time = no_opt_load_end - no_opt_load_start\n",
    "print(f\"No Opt model loading time: {no_opt_load_time:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343b6c5",
   "metadata": {},
   "source": [
    "The online optimization model loading took 4 times more time, then when you du that before saving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc54b2d",
   "metadata": {},
   "source": [
    "### 2. Inference for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e6f3aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Inference ONNX CPU optimized before save: 0.2621 s\n",
      "Time Inference ONNX CPU optimized after model load: 0.1251 s\n"
     ]
    }
   ],
   "source": [
    "inputs_onnx = {\n",
    "    \"input_ids\": encoded_inputs[\"input_ids\"].cpu().numpy(),\n",
    "    \"attention_mask\": encoded_inputs[\"attention_mask\"].cpu().numpy()\n",
    "}\n",
    "\n",
    "\n",
    "opt_time = measure_inference_time_onnx(ort_session_optimized, inputs_onnx)\n",
    "print(f\"Time Inference ONNX CPU optimized before save: {opt_time:.4f} s\")\n",
    "\n",
    "non_opt_time = measure_inference_time_onnx(ort_no_opt_session, inputs_onnx)\n",
    "print(f\"Time Inference ONNX CPU optimized after model load: {non_opt_time:.4f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e7c1f",
   "metadata": {},
   "source": [
    "There is huge difference: Model which was optimized after loading outperformed the one with optimization before saving. \n",
    "\n",
    "Furthermore, it is faster than on torch."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Docker deplyoment\n",
    "\n",
    "The pytorch app deployment is in the directory \"torch_app\".\n",
    "\n",
    "The onnx app deployment is in the directory \"onnx_app\"."
   ],
   "id": "69d2be2fb64689ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Comparision\n",
    "\n",
    "Builded container weights 988MB.\n",
    "\n",
    "Builded container weights 1.38GB.\n",
    "\n",
    "Command:\n",
    "```bash\n",
    "docker images\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "| IMAGE            | ID             | DISK USAGE | CONTENT SIZE | EXTRA |\n",
    "|-----------------|----------------|------------|--------------|-------|\n",
    "| onnx-app:latest  | a529f8ed6d24   | 988MB      | 332MB        | U     |\n",
    "| torch-app:latest | 237bcf77cd19   | 1.88GB     | 419MB        | U     |\n"
   ],
   "id": "7cc5cc2e5cefaf7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
