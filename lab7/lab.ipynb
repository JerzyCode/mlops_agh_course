{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33097f9b",
   "metadata": {},
   "source": [
    "# Lab 07 MLops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78c8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 21 16:12:15 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.274.02             Driver Version: 535.274.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:5B:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b196e7d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac18d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa96556",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef76194",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67301412",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44216997",
   "metadata": {},
   "source": [
    "### Sample input - used for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6f24d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: Hello World1!\n",
      "TOKENS: ['<s>', 'hello', 'world', '##1', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "IDS:[0, 7596, 2092, 2491, 1003, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "TEXT: Java is better programming language than python - change my mind.\n",
      "TOKENS: ['<s>', 'java', 'is', 'better', 'programming', 'language', 'than', 'python', '-', 'change', 'my', 'mind', '.', '</s>']\n",
      "IDS:[0, 9266, 2007, 2492, 4734, 2657, 2088, 18754, 1015, 2693, 2030, 2572, 1016, 2]\n",
      "\n",
      "TEXT: Transformers are powerful models for NLP.\n",
      "TOKENS: ['<s>', 'transformers', 'are', 'powerful', 'models', 'for', 'nl', '##p', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "IDS:[0, 19085, 2028, 3932, 4279, 2009, 17957, 2365, 1016, 2, 1, 1, 1, 1]\n",
      "\n",
      "TEXT: Batch processing allows multiple sentences to be encoded together.\n",
      "TOKENS: ['<s>', 'batch', 'processing', 'allows', 'multiple', 'sentences', 'to', 'be', 'encoded', 'together', '.', '</s>', '<pad>', '<pad>']\n",
      "IDS:[0, 14112, 6368, 4477, 3678, 11750, 2004, 2026, 12363, 2366, 1016, 2, 1, 1]\n",
      "\n",
      "TEXT: This is another example input.\n",
      "TOKENS: ['<s>', 'this', 'is', 'another', 'example', 'input', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "IDS:[0, 2027, 2007, 2182, 2746, 7957, 1016, 2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Hello World1!\",\n",
    "    \"Java is better programming language than python - change my mind.\",\n",
    "    \"Transformers are powerful models for NLP.\",\n",
    "    \"Batch processing allows multiple sentences to be encoded together.\",\n",
    "    \"This is another example input.\"\n",
    "]\n",
    "\n",
    "encoded_inputs = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "\n",
    "for text, ids in zip(texts, input_ids):\n",
    "    token_list = tokenizer.convert_ids_to_tokens(ids.tolist())\n",
    "    print(f\"\\nTEXT: {text}\\nTOKENS: {token_list}\\nIDS:{ids.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6aa91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs_cpu = {k: v.to(\"cpu\") for k, v in encoded_inputs.items()}\n",
    "encoded_inputs_gpu = {k: v.to(\"cuda\") for k, v in encoded_inputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969eba77",
   "metadata": {},
   "source": [
    "### Measuring inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85232916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, encoded_inputs, runs=200):\n",
    "    _ = model(**encoded_inputs)  # warmup\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for _ in range(runs):\n",
    "        outputs = model(**encoded_inputs)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    return total_time / runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f2ebe",
   "metadata": {},
   "source": [
    "#### Plain Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb305c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Plain Torch CPU: 0.2303 s\n",
      "Time Plain Torch GPU: 0.0081 s\n"
     ]
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "plain_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "print(f\"Time Plain Torch CPU: {plain_cpu:.4f} s\")\n",
    "\n",
    "model.to(device=\"cuda\")\n",
    "plain_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "print(f\"Time Plain Torch GPU: {plain_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54d802",
   "metadata": {},
   "source": [
    "#### Eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8b8b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Eval Torch CPU: 0.2332 s\n",
      "Time Eval Torch GPU: 0.0080 s\n"
     ]
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "model.eval()\n",
    "eval_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "print(f\"Time Eval Torch CPU: {eval_cpu:.4f} s\")\n",
    "\n",
    "model.to(device=\"cuda\")\n",
    "model.eval()\n",
    "eval_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "print(f\"Time Eval Torch GPU: {eval_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c9216",
   "metadata": {},
   "source": [
    "#### Eval mode and no grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1130ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Eval and no Grad Torch CPU: 0.2250 s\n",
      "Time Eval and no Grad Torch GPU: 0.0063 s\n"
     ]
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_no_grad_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "    print(f\"Time Eval and no Grad Torch CPU: {eval_no_grad_cpu:.4f} s\")\n",
    "\n",
    "model.to(device=\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_no_grad_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "    print(f\"Time Eval and no Grad Torch GPU: {eval_no_grad_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113b7ba",
   "metadata": {},
   "source": [
    "#### Full inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73277c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Inference Mode CPU: 0.2214 s\n",
      "Time Inference Mode GPU: 0.0056 s\n"
     ]
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "with torch.inference_mode():\n",
    "    eval_inference_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "    print(f\"Time Inference Mode CPU: {eval_inference_cpu:.4f} s\")\n",
    "\n",
    "model.to(device=\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    eval_inference_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "    print(f\"Time Inference Mode GPU: {eval_inference_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293a553",
   "metadata": {},
   "source": [
    "### Result of experiment\n",
    "\n",
    "| Method                     | CPU Time (s) | GPU Time (s) |\n",
    "|----------------------------|-------------|-------------|\n",
    "| Plain Torch                | 0.2303      | 0.0081      |\n",
    "| Eval Torch                 | 0.2332      | 0.0080      |\n",
    "| Eval Torch (no Grad)       | 0.2250      | 0.0063      |\n",
    "| Inference Mode             | 0.2214      | 0.0056      |\n",
    "\n",
    "\n",
    "The fastest is the inference mode. Interestingly, the Eval Torch on CPU is slower than plan torch on CPU. For the GPU Difference is small, but eval is faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8a2c2",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Compiling only for GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791e0af",
   "metadata": {},
   "source": [
    "### 1. Compiling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d436a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device=\"cuda\")\n",
    "compiled_model_gpu = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082baaa5",
   "metadata": {},
   "source": [
    "### 2. Measuring time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c331521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Compiled GPU: 0.0048 s\n"
     ]
    }
   ],
   "source": [
    "eval_compiled_gpu = measure_inference_time(compiled_model_gpu, encoded_inputs_gpu)\n",
    "print(f\"Time Compiled GPU: {eval_compiled_gpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3e1aa",
   "metadata": {},
   "source": [
    "### 3. Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6875\n"
     ]
    }
   ],
   "source": [
    "speed_up = 0.0081 / 0.0048\n",
    "print(speed_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd7f74",
   "metadata": {},
   "source": [
    "Compiled model is faster than all of previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef3ecc",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5166ab",
   "metadata": {},
   "source": [
    "### 1. Ensuring model on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2909e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device=\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca867020",
   "metadata": {},
   "source": [
    "### 2. Quantize model\n",
    "\n",
    "### 3. Save model to variable, veryfing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e1ad2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model:\n",
      "MPNetModel(\n",
      "  (embeddings): MPNetEmbeddings(\n",
      "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): MPNetEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MPNetLayer(\n",
      "        (attention): MPNetAttention(\n",
      "          (attn): MPNetSelfAttention(\n",
      "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (intermediate): MPNetIntermediate(\n",
      "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MPNetOutput(\n",
      "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (relative_attention_bias): Embedding(32, 12)\n",
      "  )\n",
      "  (pooler): MPNetPooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.ao.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "model_quantized = torch.ao.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"Quantized model:\")\n",
    "print(model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffbfdf",
   "metadata": {},
   "source": [
    "### 4. Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd6d2366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 417.73 MB\n",
      "Quantized model size: 173.10 MB\n",
      "Compression ratio: 2.41×\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "torch.save(model.state_dict(), \"model_original.pt\")\n",
    "torch.save(model_quantized.state_dict(), \"model_quantized.pt\")\n",
    "\n",
    "size_original = os.path.getsize(\"model_original.pt\")\n",
    "size_quantized = os.path.getsize(\"model_quantized.pt\")\n",
    "\n",
    "size_original_mb = size_original / (1024 * 1024)\n",
    "size_quantized_mb = size_quantized / (1024 * 1024)\n",
    "\n",
    "print(f\"Original model size: {size_original_mb:.2f} MB\")\n",
    "print(f\"Quantized model size: {size_quantized_mb:.2f} MB\")\n",
    "print(f\"Compression ratio: {size_original_mb / size_quantized_mb:.2f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27026ee0",
   "metadata": {},
   "source": [
    "### 5. Inference comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7ce6cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Quantized CPU: 0.0626 s\n",
      "Time Non Quantized CPU: 0.2359 s\n"
     ]
    }
   ],
   "source": [
    "quantized_cpu = measure_inference_time(model_quantized, encoded_inputs_cpu)\n",
    "print(f\"Time Quantized CPU: {quantized_cpu:.4f} s\")\n",
    "\n",
    "no_quantized_cpu = measure_inference_time(model, encoded_inputs_cpu)\n",
    "print(f\"Time Non Quantized CPU: {no_quantized_cpu:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0bb2e0",
   "metadata": {},
   "source": [
    "### 6. Comments on comparission\n",
    "\n",
    "Quantized model has size of 173.10MB while original 417.73MB which is over 2.41 more. Furthermore, quantized model is almost 4 times faster than plain model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c359e5",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46a517",
   "metadata": {},
   "source": [
    "### 1. Comparing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb662b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device=\"cuda\")\n",
    "compiled_model_gpu = torch.compile(model)\n",
    "compiled_model_gpu_max_autotune = torch.compile(model, mode=\"max-autotune\")\n",
    "compiled_model_gpu_max_autotune_no_cuda_graphs = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549ddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    _ = compiled_model_gpu(**encoded_inputs_gpu)\n",
    "    _ = compiled_model_gpu_max_autotune(**encoded_inputs_gpu)\n",
    "    _ = compiled_model_gpu_max_autotune_no_cuda_graphs(**encoded_inputs_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017b4e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Compiled GPU (inference_mode): 0.0031 s\n",
      "Time Compiled GPU Max Autotune (inference_mode): 0.0022 s\n",
      "Time Compiled GPU Max Autotune No CUDA Graphs (inference_mode): 0.0032 s\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    eval_compiled_gpu = measure_inference_time(compiled_model_gpu, encoded_inputs_gpu)\n",
    "    print(f\"Time Compiled GPU (inference_mode): {eval_compiled_gpu:.4f} s\")\n",
    "\n",
    "    eval_compiled_gpu_max_autotune = measure_inference_time(compiled_model_gpu_max_autotune, encoded_inputs_gpu)\n",
    "    print(f\"Time Compiled GPU Max Autotune (inference_mode): {eval_compiled_gpu_max_autotune:.4f} s\")\n",
    "\n",
    "    eval_compiled_gpu_max_autotune_no_cuda_graphs = measure_inference_time(\n",
    "        compiled_model_gpu_max_autotune_no_cuda_graphs, encoded_inputs_gpu\n",
    "    )\n",
    "    print(\n",
    "        f\"Time Compiled GPU Max Autotune No CUDA Graphs (inference_mode): {eval_compiled_gpu_max_autotune_no_cuda_graphs:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba2d80",
   "metadata": {},
   "source": [
    "### 2. Comment on results\n",
    "\n",
    "| Model                                    | Time |\n",
    "| ---------------------------------------- | ------------- |\n",
    "| Compiled GPU (default)                   | 0.0031 s      |\n",
    "| Compiled GPU Max Autotune                | 0.0022 s      |\n",
    "| Compiled GPU Max Autotune No CUDA Graphs | 0.0032 s      |\n",
    "\n",
    "\n",
    "The fastest was Compiled GPU Max Autotune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77824bef",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a11cb",
   "metadata": {},
   "source": [
    "### 1. Check capability of GPU Tensor Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a093dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (7, 0)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92fbba",
   "metadata": {},
   "source": [
    "### 2. Measruing inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3bbd306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Full Precisionn Mode GPU: 0.0058 s\n",
      "Time HAlf Precisionn Mode GPU: 0.0057 s\n",
      "Time Autocast Precisionn Mode GPU: 0.0070 s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device=\"cuda\")\n",
    "\n",
    "model_half = model.half().to('cuda')\n",
    "model_half.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    full_precision_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "    print(f\"Time Full Precisionn Mode GPU: {full_precision_gpu:.4f} s\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    half_precision_gpu = measure_inference_time(model_half, encoded_inputs_gpu)\n",
    "    print(f\"Time HAlf Precisionn Mode GPU: {half_precision_gpu:.4f} s\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        auto_precision_gpu = measure_inference_time(model, encoded_inputs_gpu)\n",
    "        print(f\"Time Autocast Precisionn Mode GPU: {auto_precision_gpu:.4f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4edd8",
   "metadata": {},
   "source": [
    "### 3. Time comparision\n",
    "\n",
    "| Type                  | Time [s]   |\n",
    "|-----------------------|------------|\n",
    "| Full Precision (FP32) | 0.0058     |\n",
    "| Half Precision (FP16) | 0.0057     |\n",
    "| Autocast (Mixed FP16) | 0.0070     |\n",
    "\n",
    "\n",
    "I would use Half precision. It is th fastest there and it is supported well for majority of gpus. However, the test samples count and length may not be enough to test real differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d24ca",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52f9f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "def measure_inference_time_onnx(session: ort.InferenceSession, encoded_inputs: dict, runs: int = 100):\n",
    "    _ = session.run(None, encoded_inputs)  #warmup\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for _ in range(runs):\n",
    "        outputs = session.run(None, encoded_inputs)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    return total_time / runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d90c2",
   "metadata": {},
   "source": [
    "### 1. Measure online and offline model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d1771",
   "metadata": {},
   "source": [
    "**transforming to onnx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae5eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "model_cpu = model.eval().cpu()\n",
    "\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"non_opt_model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f166e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "sess_options.intra_op_num_threads = 1\n",
    "sess_options.inter_op_num_threads = 2\n",
    "sess_options.enable_cpu_mem_arena = True\n",
    "sess_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "session = ort.InferenceSession(\"non_opt_model.onnx\", sess_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f77f6c",
   "metadata": {},
   "source": [
    "**comparision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "781dbd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt model loading time: 0.2370 s\n"
     ]
    }
   ],
   "source": [
    "opt_load_start = time.perf_counter()\n",
    "sess_opt_options = ort.SessionOptions()\n",
    "sess_opt_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "sess_opt_options.intra_op_num_threads = 1\n",
    "sess_opt_options.inter_op_num_threads = 2\n",
    "sess_opt_options.enable_cpu_mem_arena = True\n",
    "\n",
    "ort_session_optimized = ort.InferenceSession(\n",
    "    \"model_optimized.onnx\",\n",
    "    sess_options=sess_opt_options,\n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "opt_load_end = time.perf_counter()\n",
    "opt_load_time = opt_load_end - opt_load_start\n",
    "print(f\"Opt model loading time: {opt_load_time:.4f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15bacdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Opt model loading time: 0.9791 s\n"
     ]
    }
   ],
   "source": [
    "no_opt_load_start = time.perf_counter()\n",
    "sess_no_opt_options = ort.SessionOptions()\n",
    "sess_no_opt_options.intra_op_num_threads = 1\n",
    "sess_no_opt_options.inter_op_num_threads = 2\n",
    "sess_no_opt_options.enable_cpu_mem_arena = True\n",
    "sess_no_opt_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "ort_no_opt_session = ort.InferenceSession(\n",
    "    \"non_opt_model.onnx\", sess_options=sess_no_opt_options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "no_opt_load_end = time.perf_counter()\n",
    "no_opt_load_time = no_opt_load_end - no_opt_load_start\n",
    "print(f\"No Opt model loading time: {no_opt_load_time:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343b6c5",
   "metadata": {},
   "source": [
    "The online optimization model loading took 4 times more time, then when you du that before saving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc54b2d",
   "metadata": {},
   "source": [
    "### 2. Inference for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e6f3aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Inference ONNX CPU optimized before save: 0.2621 s\n",
      "Time Inference ONNX CPU optimized after model load: 0.1251 s\n"
     ]
    }
   ],
   "source": [
    "inputs_onnx = {\n",
    "    \"input_ids\": encoded_inputs[\"input_ids\"].cpu().numpy(),\n",
    "    \"attention_mask\": encoded_inputs[\"attention_mask\"].cpu().numpy()\n",
    "}\n",
    "\n",
    "opt_time = measure_inference_time_onnx(ort_session_optimized, inputs_onnx)\n",
    "print(f\"Time Inference ONNX CPU optimized before save: {opt_time:.4f} s\")\n",
    "\n",
    "non_opt_time = measure_inference_time_onnx(ort_no_opt_session, inputs_onnx)\n",
    "print(f\"Time Inference ONNX CPU optimized after model load: {non_opt_time:.4f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e7c1f",
   "metadata": {},
   "source": [
    "There is huge difference: Model which was optimized after loading outperformed the one with optimization before saving. \n",
    "\n",
    "Furthermore, it is faster than on torch."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Docker deplyoment\n",
    "\n",
    "The pytorch app deployment is in the directory \"torch_app\".\n",
    "\n",
    "The onnx app deployment is in the directory \"onnx_app\"."
   ],
   "id": "69d2be2fb64689ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Comparision\n",
    "\n",
    "Builded container weights 988MB.\n",
    "\n",
    "Builded container weights 1.38GB.\n",
    "\n",
    "Command:\n",
    "```bash\n",
    "docker images\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "| IMAGE            | ID             | DISK USAGE | CONTENT SIZE | EXTRA |\n",
    "|-----------------|----------------|------------|--------------|-------|\n",
    "| onnx-app:latest  | a529f8ed6d24   | 988MB      | 332MB        | U     |\n",
    "| torch-app:latest | 237bcf77cd19   | 1.88GB     | 419MB        | U     |\n"
   ],
   "id": "7cc5cc2e5cefaf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T19:59:38.344362Z",
     "start_time": "2025-11-21T19:59:38.327541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Text GPT Generated\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Beauty is in the eye of the beholder.\",\n",
    "    \"Better late than never.\",\n",
    "    \"Birds of a feather flock together.\",\n",
    "    \"Curiosity killed the cat.\",\n",
    "    \"Do unto others as you would have them do unto you.\",\n",
    "    \"Don't count your chickens before they hatch.\",\n",
    "    \"Every cloud has a silver lining.\",\n",
    "    \"Fortune favors the bold.\",\n",
    "    \"Good things come to those who wait.\",\n",
    "    \"Honesty is the best policy.\",\n",
    "    \"Fortune favors the bold.\",\n",
    "    \"Good things come to those who wait.\",\n",
    "    \"Honesty is the best policy.\",\n",
    "    \"If it ain't broke, don't fix it.\",\n",
    "    \"Knowledge is power.\",\n",
    "    \"Laughter is the best medicine.\",\n",
    "    \"Necessity is the mother of invention.\",\n",
    "    \"No man is an island.\",\n",
    "    \"Practice makes perfect.\",\n",
    "    \"Rome wasn't built in a day.\",\n",
    "    \"The pen is mightier than the sword.\",\n",
    "    \"The early bird catches the worm.\",\n",
    "    \"Time is money.\",\n",
    "    \"Too many cooks spoil the broth.\",\n",
    "    \"Two heads are better than one.\",\n",
    "    \"When in Rome, do as the Romans do.\",\n",
    "    \"You can't judge a book by its cover.\",\n",
    "    \"A picture is worth a thousand words.\",\n",
    "    \"Brevity is the soul of wit.\"\n",
    "]\n",
    "\n",
    "\n",
    "def run_experiment(app_name: str, runs=100):\n",
    "    endpoint_url = f\"http://localhost:8000/{app_name}/predict\"\n",
    "    # warmup\n",
    "    print(\"Warmup START\")\n",
    "    for i in range(5):\n",
    "        sample_text = {\"text\": sentences[i]}\n",
    "        response = requests.post(endpoint_url, json=sample_text)\n",
    "        _ = response.json()\n",
    "\n",
    "    print(\"Warmup DONE\")\n",
    "\n",
    "    times = []\n",
    "    for i in range(runs):\n",
    "        sample_text = {\"text\": sentences[i % len(sentences)]}\n",
    "        start_time = time.perf_counter()\n",
    "        _ = requests.post(endpoint_url, json=sample_text)\n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        times.append(total_time)\n",
    "\n",
    "        print(f\"Done {i + 1}/{runs} in {total_time:.4f} s\")\n",
    "\n",
    "    return sum(times) / runs"
   ],
   "id": "b8c668e3dd3ffa14",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T20:04:56.989646Z",
     "start_time": "2025-11-21T20:04:48.334493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "onnx_app_average_response_time = run_experiment(app_name=\"onnx-app\")\n",
    "\n",
    "print(f\"Average response time ONNX app: {onnx_app_average_response_time:.4f} s\")"
   ],
   "id": "2d2ab8077ac6f039",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup START\n",
      "Warmup DONE\n",
      "Done 1/100 in 0.0995 s\n",
      "Done 2/100 in 0.1074 s\n",
      "Done 3/100 in 0.1280 s\n",
      "Done 4/100 in 0.0734 s\n",
      "Done 5/100 in 0.0827 s\n",
      "Done 6/100 in 0.0891 s\n",
      "Done 7/100 in 0.0721 s\n",
      "Done 8/100 in 0.0590 s\n",
      "Done 9/100 in 0.0588 s\n",
      "Done 10/100 in 0.1116 s\n",
      "Done 11/100 in 0.1045 s\n",
      "Done 12/100 in 0.0792 s\n",
      "Done 13/100 in 0.0660 s\n",
      "Done 14/100 in 0.0772 s\n",
      "Done 15/100 in 0.0791 s\n",
      "Done 16/100 in 0.0674 s\n",
      "Done 17/100 in 0.0919 s\n",
      "Done 18/100 in 0.0626 s\n",
      "Done 19/100 in 0.1100 s\n",
      "Done 20/100 in 0.0439 s\n",
      "Done 21/100 in 0.0633 s\n",
      "Done 22/100 in 0.0793 s\n",
      "Done 23/100 in 0.0785 s\n",
      "Done 24/100 in 0.0587 s\n",
      "Done 25/100 in 0.0700 s\n",
      "Done 26/100 in 0.0681 s\n",
      "Done 27/100 in 0.0777 s\n",
      "Done 28/100 in 0.0582 s\n",
      "Done 29/100 in 0.1068 s\n",
      "Done 30/100 in 0.0935 s\n",
      "Done 31/100 in 0.0963 s\n",
      "Done 32/100 in 0.0736 s\n",
      "Done 33/100 in 0.0673 s\n",
      "Done 34/100 in 0.0659 s\n",
      "Done 35/100 in 0.0891 s\n",
      "Done 36/100 in 0.0938 s\n",
      "Done 37/100 in 0.0930 s\n",
      "Done 38/100 in 0.0633 s\n",
      "Done 39/100 in 0.0507 s\n",
      "Done 40/100 in 0.0922 s\n",
      "Done 41/100 in 0.0738 s\n",
      "Done 42/100 in 0.1011 s\n",
      "Done 43/100 in 0.0842 s\n",
      "Done 44/100 in 0.1208 s\n",
      "Done 45/100 in 0.0921 s\n",
      "Done 46/100 in 0.0616 s\n",
      "Done 47/100 in 0.0553 s\n",
      "Done 48/100 in 0.0896 s\n",
      "Done 49/100 in 0.0731 s\n",
      "Done 50/100 in 0.0732 s\n",
      "Done 51/100 in 0.1030 s\n",
      "Done 52/100 in 0.0736 s\n",
      "Done 53/100 in 0.1276 s\n",
      "Done 54/100 in 0.0511 s\n",
      "Done 55/100 in 0.0687 s\n",
      "Done 56/100 in 0.0669 s\n",
      "Done 57/100 in 0.0668 s\n",
      "Done 58/100 in 0.0590 s\n",
      "Done 59/100 in 0.0920 s\n",
      "Done 60/100 in 0.0888 s\n",
      "Done 61/100 in 0.0748 s\n",
      "Done 62/100 in 0.0580 s\n",
      "Done 63/100 in 0.0624 s\n",
      "Done 64/100 in 0.0795 s\n",
      "Done 65/100 in 0.0892 s\n",
      "Done 66/100 in 0.1030 s\n",
      "Done 67/100 in 0.0954 s\n",
      "Done 68/100 in 0.0729 s\n",
      "Done 69/100 in 0.0992 s\n",
      "Done 70/100 in 0.0968 s\n",
      "Done 71/100 in 0.1250 s\n",
      "Done 72/100 in 0.0890 s\n",
      "Done 73/100 in 0.0664 s\n",
      "Done 74/100 in 0.0811 s\n",
      "Done 75/100 in 0.0552 s\n",
      "Done 76/100 in 0.0764 s\n",
      "Done 77/100 in 0.0672 s\n",
      "Done 78/100 in 0.1011 s\n",
      "Done 79/100 in 0.0975 s\n",
      "Done 80/100 in 0.0682 s\n",
      "Done 81/100 in 0.0626 s\n",
      "Done 82/100 in 0.0832 s\n",
      "Done 83/100 in 0.0735 s\n",
      "Done 84/100 in 0.0563 s\n",
      "Done 85/100 in 0.0600 s\n",
      "Done 86/100 in 0.0667 s\n",
      "Done 87/100 in 0.1297 s\n",
      "Done 88/100 in 0.0477 s\n",
      "Done 89/100 in 0.0652 s\n",
      "Done 90/100 in 0.0753 s\n",
      "Done 91/100 in 0.0719 s\n",
      "Done 92/100 in 0.0514 s\n",
      "Done 93/100 in 0.0780 s\n",
      "Done 94/100 in 0.0741 s\n",
      "Done 95/100 in 0.0659 s\n",
      "Done 96/100 in 0.1165 s\n",
      "Done 97/100 in 0.2196 s\n",
      "Done 98/100 in 0.1391 s\n",
      "Done 99/100 in 0.0977 s\n",
      "Done 100/100 in 0.1027 s\n",
      "Average response time ONNX app: 0.0822 s\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T20:00:16.099332Z",
     "start_time": "2025-11-21T19:59:44.492788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch_app_average_response_time = run_experiment(app_name=\"torch-app\")\n",
    "\n",
    "print(f\"Average response time Torch app: {torch_app_average_response_time:.4f} s\")"
   ],
   "id": "4ff8462a249214d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup START\n",
      "Warmup DONE\n",
      "Done 1/100 in 0.0685 s\n",
      "Done 2/100 in 0.0690 s\n",
      "Done 3/100 in 0.0721 s\n",
      "Done 4/100 in 0.0778 s\n",
      "Done 5/100 in 0.0920 s\n",
      "Done 6/100 in 0.1249 s\n",
      "Done 7/100 in 0.1003 s\n",
      "Done 8/100 in 0.1243 s\n",
      "Done 9/100 in 0.1054 s\n",
      "Done 10/100 in 0.1319 s\n",
      "Done 11/100 in 0.1005 s\n",
      "Done 12/100 in 0.0810 s\n",
      "Done 13/100 in 0.0792 s\n",
      "Done 14/100 in 0.0746 s\n",
      "Done 15/100 in 0.0706 s\n",
      "Done 16/100 in 0.0679 s\n",
      "Done 17/100 in 0.1084 s\n",
      "Done 18/100 in 0.0816 s\n",
      "Done 19/100 in 0.1202 s\n",
      "Done 20/100 in 0.0662 s\n",
      "Done 21/100 in 0.0626 s\n",
      "Done 22/100 in 0.0646 s\n",
      "Done 23/100 in 0.0814 s\n",
      "Done 24/100 in 0.0689 s\n",
      "Done 25/100 in 0.0876 s\n",
      "Done 26/100 in 0.1041 s\n",
      "Done 27/100 in 0.0752 s\n",
      "Done 28/100 in 0.0532 s\n",
      "Done 29/100 in 0.1055 s\n",
      "Done 30/100 in 0.0944 s\n",
      "Done 31/100 in 0.0773 s\n",
      "Done 32/100 in 0.0903 s\n",
      "Done 33/100 in 0.0636 s\n",
      "Done 34/100 in 0.0674 s\n",
      "Done 35/100 in 0.0676 s\n",
      "Done 36/100 in 0.0716 s\n",
      "Done 37/100 in 0.0904 s\n",
      "Done 38/100 in 0.0873 s\n",
      "Done 39/100 in 0.0579 s\n",
      "Done 40/100 in 0.0770 s\n",
      "Done 41/100 in 0.0653 s\n",
      "Done 42/100 in 0.0549 s\n",
      "Done 43/100 in 0.0708 s\n",
      "Done 44/100 in 0.0988 s\n",
      "Done 45/100 in 0.0884 s\n",
      "Done 46/100 in 0.0838 s\n",
      "Done 47/100 in 0.0639 s\n",
      "Done 48/100 in 0.0655 s\n",
      "Done 49/100 in 0.0622 s\n",
      "Done 50/100 in 0.0664 s\n",
      "Done 51/100 in 0.0870 s\n",
      "Done 52/100 in 0.0756 s\n",
      "Done 53/100 in 0.0994 s\n",
      "Done 54/100 in 0.0491 s\n",
      "Done 55/100 in 0.0623 s\n",
      "Done 56/100 in 0.0652 s\n",
      "Done 57/100 in 0.0585 s\n",
      "Done 58/100 in 0.0645 s\n",
      "Done 59/100 in 0.0827 s\n",
      "Done 60/100 in 0.0775 s\n",
      "Done 61/100 in 0.0850 s\n",
      "Done 62/100 in 0.0638 s\n",
      "Done 63/100 in 0.0718 s\n",
      "Done 64/100 in 0.0654 s\n",
      "Done 65/100 in 0.0716 s\n",
      "Done 66/100 in 0.1000 s\n",
      "Done 67/100 in 0.1059 s\n",
      "Done 68/100 in 0.2643 s\n",
      "Done 69/100 in 0.0892 s\n",
      "Done 70/100 in 0.0873 s\n",
      "Done 71/100 in 0.1556 s\n",
      "Done 72/100 in 0.1684 s\n",
      "Done 73/100 in 0.1417 s\n",
      "Done 74/100 in 0.1298 s\n",
      "Done 75/100 in 0.1272 s\n",
      "Done 76/100 in 0.1025 s\n",
      "Done 77/100 in 0.1085 s\n",
      "Done 78/100 in 0.1704 s\n",
      "Done 79/100 in 0.2084 s\n",
      "Done 80/100 in 0.1477 s\n",
      "Done 81/100 in 0.1239 s\n",
      "Done 82/100 in 0.1501 s\n",
      "Done 83/100 in 0.0853 s\n",
      "Done 84/100 in 0.0736 s\n",
      "Done 85/100 in 0.0823 s\n",
      "Done 86/100 in 0.0977 s\n",
      "Done 87/100 in 0.1092 s\n",
      "Done 88/100 in 0.0580 s\n",
      "Done 89/100 in 0.1098 s\n",
      "Done 90/100 in 0.0971 s\n",
      "Done 91/100 in 0.0812 s\n",
      "Done 92/100 in 0.1098 s\n",
      "Done 93/100 in 0.1566 s\n",
      "Done 94/100 in 0.1790 s\n",
      "Done 95/100 in 0.3447 s\n",
      "Done 96/100 in 0.1670 s\n",
      "Done 97/100 in 0.1811 s\n",
      "Done 98/100 in 0.1360 s\n",
      "Done 99/100 in 0.2460 s\n",
      "Done 100/100 in 0.1987 s\n",
      "Average response time Torch app: 0.1006 s\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Comment on results**\n",
    "\n",
    "Average time for ONNX app: 0.0822s\n",
    "\n",
    "Average time for Torch app: 0.1006s\n",
    "\n",
    "\n",
    "It is worth to convert model to onnx, not only container is ligher but also inference time is smaller."
   ],
   "id": "f6e7365fa0c710e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
