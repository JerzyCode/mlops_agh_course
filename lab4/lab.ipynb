{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4decbc",
   "metadata": {},
   "source": [
    "## Lab 04 Vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9520b7cb",
   "metadata": {},
   "source": [
    "# PG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31eee60",
   "metadata": {},
   "source": [
    "### Creating db_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa35849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.engine import URL\n",
    "\n",
    "db_url = URL.create(\n",
    "    drivername=\"postgresql+psycopg\",\n",
    "    username=\"postgres\",\n",
    "    password=\"password\",\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    database=\"similarity_search_service_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eceb43",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f949755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy import Integer, String\n",
    "from typing import List\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n",
    "\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    __abstract__ = True\n",
    "\n",
    "\n",
    "class Image(Base):\n",
    "    __tablename__ = \"images\"\n",
    "    VECTOR_LENGTH = 512\n",
    "\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    image_path: Mapped[str] = mapped_column(String(256))\n",
    "    image_embedding: Mapped[List[float]] = mapped_column(Vector(VECTOR_LENGTH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d231631",
   "metadata": {},
   "source": [
    "### Connecting to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc2feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634307f8",
   "metadata": {},
   "source": [
    "### Creating Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5a2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659dbe10",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f37f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Session\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def insert_image(\n",
    "    engine: sqlalchemy.Engine, image_path: str, image_embedding: list[float]\n",
    "):\n",
    "    with Session(engine) as session:\n",
    "        image = Image(image_path=image_path, image_embedding=image_embedding)\n",
    "        session.add(image)\n",
    "        session.commit()\n",
    "\n",
    "\n",
    "N = 100\n",
    "for i in range(N):\n",
    "    image_path = f\"image_{i}.jpg\"\n",
    "    image_embedding = np.random.rand(512).tolist()\n",
    "    insert_image(engine, image_path, image_embedding)\n",
    "\n",
    "with Session(engine) as session:\n",
    "    image = session.query(Image).first()\n",
    "\n",
    "\n",
    "def find_k_images(\n",
    "    engine: sqlalchemy.Engine, k: int, orginal_image: Image\n",
    ") -> list[Image]:\n",
    "    with Session(engine) as session:\n",
    "        # execution_options={\"prebuffer_rows\": True} is used to prebuffer the rows, this is useful when we want to fetch the rows in chunks and return them after session is closed\n",
    "        result = session.execute(\n",
    "            sqlalchemy.select(Image)\n",
    "            .order_by(\n",
    "                Image.image_embedding.cosine_distance(orginal_image.image_embedding)\n",
    "            )\n",
    "            .limit(k),\n",
    "            execution_options={\"prebuffer_rows\": True},\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "k = 10\n",
    "similar_images = find_k_images(engine, k, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f001d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_0.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(similar_images)[0][0].image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4310c6",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678f0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "\n",
    "def find_images_with_similarity_score_greater_than(\n",
    "    engine: sqlalchemy.Engine, similarity_score: float, orginal_image: Image\n",
    ") -> list[Image]:\n",
    "    with Session(engine) as session:\n",
    "        result = session.execute(\n",
    "            select(Image).filter(\n",
    "                Image.image_embedding.cosine_distance(orginal_image.image_embedding)\n",
    "                > similarity_score\n",
    "            ),\n",
    "            execution_options={\"prebuffer_rows\": True},\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af34c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ChunkedIteratorResult at 0x735775da5590>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image(image_path=\"test_path.jpg\", image_embedding=np.random.rand(512).tolist())\n",
    "\n",
    "find_images_with_similarity_score_greater_than(\n",
    "    engine, similarity_score=0.65, orginal_image=img\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f209c10",
   "metadata": {},
   "source": [
    "## Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759c7bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e409d636eb354ff495d8acc86de6e4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1c21bb035545579b452130ab8ac94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-e2ed184370a069(…):   0%|          | 0.00/123M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ee46748eea4382aeb30dc3bf002a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/83560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AppID': Value('int64'), 'Name': Value('string'), 'Release date': Value('string'), 'Estimated owners': Value('string'), 'Peak CCU': Value('int64'), 'Required age': Value('int64'), 'Price': Value('float64'), 'DLC count': Value('int64'), 'About the game': Value('string'), 'Supported languages': Value('string'), 'Full audio languages': Value('string'), 'Reviews': Value('string'), 'Header image': Value('string'), 'Website': Value('string'), 'Support url': Value('string'), 'Support email': Value('string'), 'Windows': Value('bool'), 'Mac': Value('bool'), 'Linux': Value('bool'), 'Metacritic score': Value('int64'), 'Metacritic url': Value('string'), 'User score': Value('int64'), 'Positive': Value('int64'), 'Negative': Value('int64'), 'Score rank': Value('float64'), 'Achievements': Value('int64'), 'Recommendations': Value('int64'), 'Notes': Value('string'), 'Average playtime forever': Value('int64'), 'Average playtime two weeks': Value('int64'), 'Median playtime forever': Value('int64'), 'Median playtime two weeks': Value('int64'), 'Developers': Value('string'), 'Publishers': Value('string'), 'Categories': Value('string'), 'Genres': Value('string'), 'Tags': Value('string'), 'Screenshots': Value('string'), 'Movies': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FronkonGames/steam-games-dataset\")\n",
    "\n",
    "columns = dataset[\"train\"].features\n",
    "print(columns)\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"Name\",\n",
    "    \"Windows\",\n",
    "    \"Linux\",\n",
    "    \"Mac\",\n",
    "    \"About the game\",\n",
    "    \"Supported languages\",\n",
    "    \"Price\",\n",
    "]\n",
    "\n",
    "N = 40000\n",
    "dataset = dataset[\"train\"].select_columns(columns_to_keep).select(range(N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6aa666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Integer, Float, Boolean\n",
    "\n",
    "\n",
    "class Games(Base):\n",
    "    __tablename__ = \"games\"\n",
    "    __table_args__ = {\"extend_existing\": True}\n",
    "\n",
    "    # the vector size produced by the model taken from documentation https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2\n",
    "    VECTOR_LENGTH = 512  # check the model output dimensionality\n",
    "\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    name: Mapped[str] = mapped_column(String(256))\n",
    "    description: Mapped[str] = mapped_column(String(4096))\n",
    "    windows: Mapped[bool] = mapped_column(Boolean)\n",
    "    linux: Mapped[bool] = mapped_column(Boolean)\n",
    "    mac: Mapped[bool] = mapped_column(Boolean)\n",
    "    price: Mapped[float] = mapped_column(Float)\n",
    "    game_description_embedding: Mapped[List[float]] = mapped_column(\n",
    "        Vector(VECTOR_LENGTH)\n",
    "    )\n",
    "\n",
    "\n",
    "Base.metadata.drop_all(engine)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94217f4a",
   "metadata": {},
   "source": [
    "### Sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d3e4aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5005654c67c44d2a9459fd3cd4eef341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/341 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3069fa4c0a24eb59f5c0de8f80b9c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4d82ff91ab4e4d81e30be56ce4c029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a2760c3b104285894d01e25c99be2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e177d247a24fb8ba95a623bfe71c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a08254a3ab42029716bd446bf0acac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9011830539a34d2590a238b72d907e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263e880757a54178a81a5558799f59dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886854790f9e4b39ab328ada01d00641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f976221d40f400f80da4a2e4203aa48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4574174090ce49459ab0b9bd58ee3246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7fa163e2a842d9a484b0f12c485bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a665962cfcff4bd7a0e876c69057e770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/model.safetensors:   0%|          | 0.00/1.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "checkpoint = \"distiluse-base-multilingual-cased-v2\"\n",
    "model = SentenceTransformer(checkpoint, device=\"cpu\")\n",
    "\n",
    "\n",
    "def generate_embeddings(text: str) -> list[float]:\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47a1b2",
   "metadata": {},
   "source": [
    "### Insert games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4092f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def insert_games(engine, dataset):\n",
    "    with tqdm(total=len(dataset)) as pbar:\n",
    "        for i, game in enumerate(dataset):\n",
    "            game_description = game[\"About the game\"] or \"\"\n",
    "            game_embedding = generate_embeddings(game_description)\n",
    "            name, windows, linux, mac, price = (\n",
    "                game[\"Name\"],\n",
    "                game[\"Windows\"],\n",
    "                game[\"Linux\"],\n",
    "                game[\"Mac\"],\n",
    "                game[\"Price\"],\n",
    "            )\n",
    "            if name and windows and linux and mac and price and game_description:\n",
    "                game = Games(\n",
    "                    name=game[\"Name\"],\n",
    "                    description=game_description[0:4096],\n",
    "                    windows=game[\"Windows\"],\n",
    "                    linux=game[\"Linux\"],\n",
    "                    mac=game[\"Mac\"],\n",
    "                    price=game[\"Price\"],\n",
    "                    game_description_embedding=game_embedding,\n",
    "                )\n",
    "                with Session(engine) as session:\n",
    "                    session.add(game)\n",
    "                    session.commit()\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57584ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:03:02<00:00, 10.58it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_games(engine, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec063b3",
   "metadata": {},
   "source": [
    "### Find games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1879cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def find_game(\n",
    "    engine: sqlalchemy.Engine,\n",
    "    game_description: str,\n",
    "    windows: Optional[bool] = None,\n",
    "    linux: Optional[bool] = None,\n",
    "    mac: Optional[bool] = None,\n",
    "    price: Optional[int] = None,\n",
    "):\n",
    "    with Session(engine) as session:\n",
    "        game_embedding = generate_embeddings(game_description)\n",
    "\n",
    "        query = select(Games).order_by(\n",
    "            Games.game_description_embedding.cosine_distance(game_embedding)\n",
    "        )\n",
    "\n",
    "        if price:\n",
    "            query = query.filter(Games.price <= price)\n",
    "        if windows:\n",
    "            query = query.filter(Games.windows)\n",
    "        if linux:\n",
    "            query = query.filter(Games.linux)\n",
    "        if mac:\n",
    "            query = query.filter(Games.mac)\n",
    "\n",
    "        result = session.execute(query, execution_options={\"prebuffer_rows\": True})\n",
    "        game = result.scalars().first()\n",
    "\n",
    "        return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3c702ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: Ultimate Spider Hero\n",
      "Description: Ultimate Spider Hero game was designed for real heroes! Your mission is to help poor residents of the Metropolis and to save them from the terrible monsters. Move forward to fight your enemies and try not to fall! Features: Simple and addictive gameplay Nice graphics Awesome Ultimate Spider Hero Countless Steam achievements for you to collect! Compatibility with multiple major platforms (Windows, Mac, Linux, SteamOS) Make your way through the endless labyrinths of long, confusing city streets together with your favorite hero from countless movies and cartoons! Although this may look simple enough, things are not as easy as they seem. You will have to learn how to cling into houses properly using your web, otherwise you will fall to your demise. If you manage to do so - you will become a real superhero, armed with elusiveness, agility and speed and the ability to tirelessly swing across the rooftops and between the huge skyscrapers this urban landscape has to offer in this thrilling game of a super spider. It's fun, the visuals are magnificent and the controls are simple!\n",
      "Game: 3D PUZZLE - Modern House\n",
      "Description: Collect a 3D puzzle, transferring things to the right places to create a beautiful house. You need to go to the item, take it by pressing the left mouse button and take the item to the desired location marked in green. If you brought the correct item, it will snap into place and you will receive leaderboard points and achievements for this. Collect as much substance as possible as quickly as possible to get more points for the leaderboard. If you brought the wrong item, you can throw it away, it will return to the starting location so that you can pick it up again.\n"
     ]
    }
   ],
   "source": [
    "game = find_game(engine, \"This is a game about a hero who saves the world\", price=10)\n",
    "print(f\"Game: {game.name}\")\n",
    "print(f\"Description: {game.description}\")\n",
    "\n",
    "game = find_game(engine, game_description=\"Home decorating\", price=20)\n",
    "print(f\"Game: {game.name}\")\n",
    "print(f\"Description: {game.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eae615c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 3D PUZZLE - Old House\n",
      "Description: Collect a 3D puzzle, transferring things to the right places to create a beautiful house. You need to go to the item, take it by pressing the left mouse button and take the item to the desired location marked in green. If you brought the correct item, it will snap into place and you will receive leaderboard points and achievements for this. Collect as much substance as possible as quickly as possible to get more points for the leaderboard. If you brought the wrong item, you can throw it away, it will return to the starting location so that you can pick it up again.\n"
     ]
    }
   ],
   "source": [
    "game = find_game(engine, game_description=\"Home decorating\", mac=True, price=5)\n",
    "print(f\"Game: {game.name}\")\n",
    "print(f\"Description: {game.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd46c7",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3805ec",
   "metadata": {},
   "source": [
    "### Connec to to milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "host = \"localhost\"\n",
    "port = \"19530\"\n",
    "\n",
    "milvus_client = MilvusClient(host=host, port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d295c",
   "metadata": {},
   "source": [
    "### Create schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, DataType, CollectionSchema\n",
    "\n",
    "VECTOR_LENGTH = 768  # check the dimensionality for Silver Retriever Base (v1.1) model\n",
    "\n",
    "id_field = FieldSchema(\n",
    "    name=\"id\", dtype=DataType.INT64, is_primary=True, description=\"Primary id\"\n",
    ")\n",
    "text = FieldSchema(\n",
    "    name=\"text\", dtype=DataType.VARCHAR, max_length=4096, description=\"Page text\"\n",
    ")\n",
    "embedding_text = FieldSchema(\n",
    "    \"embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=VECTOR_LENGTH,\n",
    "    description=\"Embedded text\",\n",
    ")\n",
    "\n",
    "fields = [id_field, text, embedding_text]\n",
    "\n",
    "schema = CollectionSchema(\n",
    "    fields=fields,\n",
    "    auto_id=True,\n",
    "    enable_dynamic_field=True,\n",
    "    description=\"RAG Texts collection\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac8b93",
   "metadata": {},
   "source": [
    "### Create collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d878102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rag_texts_and_embeddings']\n",
      "{'collection_name': 'rag_texts_and_embeddings', 'auto_id': True, 'num_shards': 1, 'description': 'RAG Texts collection', 'fields': [{'field_id': 100, 'name': 'id', 'description': 'Primary id', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'text', 'description': 'Page text', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 4096}}, {'field_id': 102, 'name': 'embedding', 'description': 'Embedded text', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'functions': [], 'aliases': [], 'collection_id': 461835363952623831, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True, 'created_timestamp': 461835374861484038}\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"rag_texts_and_embeddings\"\n",
    "\n",
    "milvus_client.create_collection(collection_name=COLLECTION_NAME, schema=schema)\n",
    "\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_type=\"HNSW\",\n",
    "    metric_type=\"L2\",\n",
    "    params={\"M\": 4, \"efConstruction\": 64},  # lower values for speed\n",
    ")\n",
    "\n",
    "\n",
    "milvus_client.create_index(collection_name=COLLECTION_NAME, index_params=index_params)\n",
    "\n",
    "# checkout our collection\n",
    "print(milvus_client.list_collections())\n",
    "\n",
    "# describe our collection\n",
    "print(milvus_client.describe_collection(COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16954ff4",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fab5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data source and destination\n",
    "## the document origin destination from which document will be downloaded\n",
    "pdf_url = \"https://www.iab.org.pl/wp-content/uploads/2024/04/Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the document\n",
    "file_name = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the processed document\n",
    "file_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.json\"\n",
    "\n",
    "## local destination of the embedded pages of the document\n",
    "embeddings_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska-Embeddings.json\"\n",
    "\n",
    "## local destination of all above local required files\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74989215",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_pdf_data(pdf_url: str, file_name: str) -> None:\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    with open(os.path.join(data_dir, file_name), \"wb\") as file:\n",
    "        for block in response.iter_content(chunk_size=1024):\n",
    "            if block:\n",
    "                file.write(block)\n",
    "\n",
    "\n",
    "download_pdf_data(pdf_url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404b73a",
   "metadata": {},
   "source": [
    "### Preparing and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_pdf_text(file_name, file_json):\n",
    "    document = fitz.open(os.path.join(data_dir, file_name))\n",
    "    pages = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        pages.append({\"page_num\": page_num, \"text\": page_text})\n",
    "\n",
    "    with open(os.path.join(data_dir, file_json), \"w\") as file:\n",
    "        json.dump(pages, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "extract_pdf_text(file_name, file_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e7919",
   "metadata": {},
   "source": [
    "### Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def generate_embeddings(file_json, embeddings_json, model):\n",
    "    pages = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for page in data:\n",
    "        pages.append(page[\"text\"])\n",
    "\n",
    "    embeddings = model.encode(pages)\n",
    "\n",
    "    embeddings_paginated = []\n",
    "    for page_num in range(len(embeddings)):\n",
    "        embeddings_paginated.append(\n",
    "            {\"page_num\": page_num, \"embedding\": embeddings[page_num].tolist()}\n",
    "        )\n",
    "\n",
    "    with open(os.path.join(data_dir, embeddings_json), \"w\") as file:\n",
    "        json.dump(embeddings_paginated, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "model_name = \"ipipan/silver-retriever-base-v1.1\"\n",
    "device = \"cpu\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "generate_embeddings(file_json, embeddings_json, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef39c90",
   "metadata": {},
   "source": [
    "### Insert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43b9f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embeddings(\n",
    "    file_json,\n",
    "    embeddings_json,\n",
    "    client=milvus_client,\n",
    "    collection_name=\"rag_texts_and_embeddings\",\n",
    "):\n",
    "    rows = []\n",
    "    with (\n",
    "        open(os.path.join(data_dir, file_json), \"r\") as t_f,\n",
    "        open(os.path.join(data_dir, embeddings_json), \"r\") as e_f,\n",
    "    ):\n",
    "        text_data, embedding_data = json.load(t_f), json.load(e_f)\n",
    "        text_data = list(map(lambda d: d[\"text\"], text_data))\n",
    "        embedding_data = list(map(lambda d: d[\"embedding\"], embedding_data))\n",
    "\n",
    "        for page, (text, embedding) in enumerate(zip(text_data, embedding_data)):\n",
    "            rows.append({\"text\": text, \"embedding\": embedding})\n",
    "\n",
    "    client.insert(collection_name=collection_name, data=rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_embeddings(file_json, embeddings_json)\n",
    "\n",
    "# load inserted data into memory\n",
    "milvus_client.load_collection(\"rag_texts_and_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff288f",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85b466a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(\n",
    "    model,\n",
    "    query,\n",
    "    client=milvus_client,\n",
    "    collection_name=\"rag_texts_and_embeddings\",\n",
    "    metric_type=\"L2\",\n",
    "):\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "    result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[embedded_query],\n",
    "        limit=1,\n",
    "        search_params={\"metric_type\": metric_type},\n",
    "        output_fields=[\"text\"],\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d294b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historia powstania\n",
      "sztucznej inteligencji\n",
      "7\n",
      "W języku potocznym „sztuczny\" oznacza to, co\n",
      "jest \n",
      "wytworem \n",
      "mającym \n",
      "naśladować \n",
      "coś\n",
      "naturalnego. W takim znaczeniu używamy\n",
      "terminu ,,sztuczny'', gdy mówimy o sztucznym\n",
      "lodowisku lub oku. Sztuczna inteligencja byłaby\n",
      "czymś (programem, maszyną) symulującym\n",
      "inteligencję naturalną, ludzką.\n",
      "Sztuczna inteligencja (AI) to obszar informatyki,\n",
      "który skupia się na tworzeniu programów\n",
      "komputerowych zdolnych do wykonywania\n",
      "zadań, które wymagają ludzkiej inteligencji. \n",
      "Te zadania obejmują rozpoznawanie wzorców,\n",
      "rozumienie języka naturalnego, podejmowanie\n",
      "decyzji, uczenie się, planowanie i wiele innych.\n",
      "Głównym celem AI jest stworzenie systemów,\n",
      "które są zdolne do myślenia i podejmowania\n",
      "decyzji na sposób przypominający ludzki.\n",
      "Historia sztucznej inteligencji sięga lat 50. \n",
      "XX wieku, kiedy to powstały pierwsze koncepcje\n",
      "i modele tego, co mogłoby stać się sztuczną\n",
      "inteligencją. Jednym z pionierów był Alan\n",
      "Turing, który sformułował test Turinga, mający\n",
      "na \n",
      "celu \n",
      "ocenę \n",
      "zdolności \n",
      "maszyny \n",
      "do\n",
      "inteligentnego \n",
      "zachowania \n",
      "na \n",
      "poziomie\n",
      "ludzkim. Jednakże dopiero w latach 80. i 90.\n",
      "nastąpił \n",
      "prawdziwy \n",
      "przełom \n",
      "w \n",
      "dziedzinie\n",
      "sztucznej \n",
      "inteligencji \n",
      "dzięki \n",
      "postępowi \n",
      "w\n",
      "dziedzinie algorytmów uczenia maszynowego.\n",
      "W wypadku sztucznej inteligencji mamy na\n",
      "uwadze system, który realizowałby niektóre\n",
      "funkcje \n",
      "umysłu \n",
      "– \n",
      "czasami \n",
      "w \n",
      "sposób\n",
      "przewyższający funkcje naturalne (na przykład,\n",
      "aby był wolny od pomyłek przy liczeniu oraz\n",
      "defektów \n",
      "pamięci). \n",
      "Inteligencja \n",
      "jest \n",
      "wła-\n",
      "ściwością umysłu. \n",
      "Składa się na nią szereg umiejętności, takich jak\n",
      "zdolność do komunikowania, rozwiązywania\n",
      "problemów, uczenia się i dostosowywania do\n",
      "sytuacji. \n",
      "Istotna \n",
      "jest \n",
      "jednak \n",
      "umiejętność\n",
      "rozumowania.\n",
      "Współczesne systemy sztucznej inteligencji są\n",
      "inteligentne tylko w ograniczonym obszarze. \n",
      "Na przykład komputer potrafi grać w szachy w\n",
      "taki \n",
      "sposób, \n",
      "że \n",
      "wygrywa \n",
      "z \n",
      "szachowym\n",
      "arcymistrzem. W 1996 r. Deep Blue wygrał jedną\n",
      "partię \n",
      "szachów \n",
      "z \n",
      "Garry \n",
      "Kasparowem,\n",
      "przegrywając cały mecz wynikiem 4:2 (przy\n",
      "dwóch remisach).\n",
      "Później Deep Blue został ulepszony i nie-\n",
      "oficjalnie \n",
      "nazwany \n",
      "„Deeper \n",
      "Blue\". \n",
      "Zagrał\n",
      "ponownie z Kasparowem w maju 1997 roku.\n",
      "Mecz \n",
      "skończył \n",
      "się \n",
      "wynikiem \n",
      "3½:2½ \n",
      "dla\n",
      "komputera. W ten sposób Deep Blue stał się\n",
      "pierwszym systemem komputerowym, który\n",
      "wygrał z aktualnym mistrzem świata w meczu\n",
      "ze standardową kontrolą czasu.\n",
      "Źródło: Midjourney – obraz wygenerowany przez AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = search(model, query=\"Czym jest sztuczna inteligencja\")\n",
    "print(result[0][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f08428",
   "metadata": {},
   "source": [
    "### Gemini API integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58336f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "051c63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "gemini_client = genai.Client(api_key=GEMINI_KEY)\n",
    "\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    try:\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "95f42a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt is in polish, because the context is in polish\n",
    "\n",
    "\n",
    "def build_prompt(context: str, query: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "        Jesteś ekspertem w dziedzinie Sztucznej Inteligencji i Uczenia Maszynowego.\n",
    "        Na podstawie dostarczonego kontekstu:\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Znajdź odpowiedź na następujące pytanie:\n",
    "        \n",
    "        {query}\n",
    "\n",
    "        Bądź bardzo precyzyjny i nie podawaj żadnych informacji, które nie znajdują się w dostarczonym kontekście.\n",
    "        Jeśli w kontekście nie ma odpowiedzi, odpowiedz \"Nie znaleziono informacji\".\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def rag(\n",
    "    model,\n",
    "    query: str,\n",
    "    collection_nane: str = \"rag_texts_and_embeddings\",\n",
    "    metric_type: str = \"L2\",\n",
    ") -> str:\n",
    "    context = search(\n",
    "        model, query=query, collection_name=collection_nane, metric_type=metric_type\n",
    "    )[0][0][\"text\"]\n",
    "    prompt = build_prompt(context, query)\n",
    "    response = generate_response(prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5330c5",
   "metadata": {},
   "source": [
    "### Having fun with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08f7613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nie znaleziono informacji.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"Czym jest Contrastive Language-Image Pretraining?\"\n",
    "response1 = rag(model, query1)\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c7d83",
   "metadata": {},
   "source": [
    "#### Debug why 'nie znaleziono informacji'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ed255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wielkość zasobów internetowych, na podstawie\n",
      "których LLM rozwija swoje możliwości, staje się\n",
      "coraz większa, a nad ich jakością czuwają\n",
      "twórcy modelu. Przełomem w ustanawianiu\n",
      "standardów z zakresu modelowania języka był\n",
      "ChatGPT-2, który pokazał, jak zwiększenie skali\n",
      "modelu może prowadzić do nowych i bardziej\n",
      "zaawansowanych możliwości. Te umiejętności\n",
      "tworzenia tekstu przez GPT-2 przełożyły się na\n",
      "zdolność do wytwarzania długich, spójnych i\n",
      "kontekstowo poprawnych treści, z tworzeniem\n",
      "esejów, opowiadań, a nawet poezji włącznie.\n",
      "Model wykazał też zdolność do rozumienia i\n",
      "rozwiązywania \n",
      "bardziej \n",
      "złożonych \n",
      "zadań\n",
      "językowych, \n",
      "takich \n",
      "jak \n",
      "rozróżnianie\n",
      "dwuznaczności \n",
      "w \n",
      "tekstach. \n",
      "Kolejne \n",
      "etapy\n",
      "rozwoju popularnego czatu doprowadziły do\n",
      "wersji \n",
      "GPT-4, \n",
      "wykraczającej \n",
      "już \n",
      "poza\n",
      "modelowanie językowe i obejmującej multi-\n",
      "media.\n",
      "Jeśli zastanawiasz się, dlaczego ciągle mówimy\n",
      "o \"przewidywaniu jednego, kolejnego słowa\",\n",
      "podczas gdy na przykład czat GPT bez trudu\n",
      "generuje długie teksty, to odpowiedź jest\n",
      "prosta: modele językowe faktycznie generują\n",
      "długie teksty, ale robią to krok po kroku, słowo\n",
      "po słowie. W rzeczywistości, po wygenerowaniu\n",
      "każdego \n",
      "nowego \n",
      "słowa, \n",
      "model \n",
      "ponownie\n",
      "przetwarza cały poprzedni tekst wraz z nowo\n",
      "dodanym \n",
      "fragmentem, \n",
      "aby \n",
      "wygenerować\n",
      "kolejne słowo. W ten sposób powstaje spójna\n",
      "wypowiedź.\n",
      "W rzeczywistości duże modele językowe (LLM)\n",
      "starają się przewidzieć nie tyle konkretne,\n",
      "następne \n",
      "słowo, \n",
      "ile \n",
      "prawdopodobieństwa\n",
      "użycia różnych słów, które mogą kontynuować\n",
      "dany tekst. Dlaczego najlepszym rozwiązaniem\n",
      "nie \n",
      "zawsze \n",
      "jest \n",
      "szukanie \n",
      "\"najbardziej\n",
      "odpowiedniego\" \n",
      "słowa? \n",
      "Rozważmy \n",
      "to \n",
      "na\n",
      "przykładzie prostej gry.\n",
      "Załóżmy, że masz kontynuować tekst: \"44.\n",
      "prezydent USA (i pierwszy Afroamerykanin na\n",
      "tym stanowisku) to Barack...\". Jeśli pomyślałeś,\n",
      "że następnym słowem powinno być \"Obama\" z\n",
      "prawdopodobieństwem 100%, to się mylisz! W\n",
      "oficjalnych \n",
      "dokumentach \n",
      "imię \n",
      "prezydenta\n",
      "często jest podawane w wersji pełnej, razem z\n",
      "drugim imieniem \"Hussein\". Dlatego dobrze\n",
      "wytrenowany LLM przewidziałby \"Obama\" z\n",
      "prawdopodobieństwem \n",
      "około \n",
      "90%, \n",
      "pozo-\n",
      "stawiając 10% dla \"Hussein\".\n",
      "To obrazuje interesujący aspekt LLM – jego\n",
      "twórczy \n",
      "charakter. \n",
      "Podczas \n",
      "generowania\n",
      "każdego następnego słowa, modele wybierają\n",
      "je \"losowo\" bazując na prawdopodobieństwach\n",
      "wyliczonych \n",
      "podczas \n",
      "treningu \n",
      "na \n",
      "dużych\n",
      "zbiorach tekstów. Dzięki temu, te same modele\n",
      "mogą dawać różne odpowiedzi na identyczne\n",
      "zapytania, podobnie jak ludzie. Eksperymenty\n",
      "pokazały, \n",
      "że \n",
      "modele \n",
      "wybierające \n",
      "zawsze\n",
      "\"najbardziej prawdopodobne\" słowo działają\n",
      "gorzej niż te, które wprowadzają element\n",
      "losowości.\n",
      "Język jest strukturą z zasadami i wyjątkami, a\n",
      "słowa w zdaniach są ze sobą powiązane. \n",
      "Ludzie uczą się tych powiązań naturalnie, a\n",
      "dobre modele językowe muszą odwzorować tę\n",
      "zmienność i bogactwo języka, aby precyzyjnie\n",
      "oceniać prawdopodobieństwa słów w zale-\n",
      "żności od kontekstu.\n",
      "W tym miejscu warto zrobić małe odstępstwo,\n",
      "by \n",
      "zastanowić \n",
      "się, \n",
      "co \n",
      "właściwie \n",
      "znaczy\n",
      "stwierdzenie, \n",
      "że \n",
      "\"model \n",
      "potrafi \n",
      "rozwiązać\n",
      "zadanie\"? Proces sprowadza się do tego, że\n",
      "podajemy modelowi tekst w formie zapytania\n",
      "(prompt), \n",
      "a \n",
      "on \n",
      "generuje \n",
      "odpowiednie\n",
      "kontynuacje. \n",
      "Jeśli \n",
      "te \n",
      "generowane \n",
      "treści\n",
      "zgadzają się z naszymi oczekiwaniami, możemy\n",
      "uznać, \n",
      "że \n",
      "model \n",
      "sprostał \n",
      "postawionemu\n",
      "zadaniu.\n",
      "Wprowadzenie do Dużych\n",
      "Modeli Językowych (LLM)\n",
      "2 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = search(model, query=query1)[0][0][\"text\"]\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5039b88",
   "metadata": {},
   "source": [
    "Context is wrong, however there is information about the query in doc.\n",
    "\n",
    "Let's try with easier query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generatywne modele grafiki komputerowej, w tym Pix2Pix, są dobre do generowania sztuki, projektowania i produkcji treści multimedialnych.\n"
     ]
    }
   ],
   "source": [
    "query2 = (\n",
    "    \"Jaki model jest dobry do generowania sztuki, np. do materiałów marketingowych?\"\n",
    ")\n",
    "response2 = rag(model, query2)\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6454935",
   "metadata": {},
   "source": [
    "For that query, model halucinated. The answer should be Blue Willow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0133a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inne modele sztucznej\n",
      "inteligencji\n",
      "1 6\n",
      "Generatywne modele grafiki\n",
      "komputerowej:\n",
      "Pix2Pix: Ten model jest wykorzystywany do\n",
      "konwersji obrazów z jednej dziedziny na obrazy\n",
      "w innej. Na przykład może przekształcać obrazy\n",
      "czarnobiałe na kolorowe, rysunki w realistyczne\n",
      "obrazy, itp.\n",
      "Sztuczna \n",
      "inteligencja \n",
      "generatywna \n",
      "ma\n",
      "potencjał zastosowania w wielu dziedzinach,\n",
      "takich jak: sztuka, projektowanie, produkcja\n",
      "treści multimedialnych, czy nawet medycyna.\n",
      "Jednakże, \n",
      "równocześnie \n",
      "z \n",
      "zastosowaniami\n",
      "pozytywnymi, pojawiają się także wyzwania\n",
      "związane z etyką i bezpieczeństwem, zwłaszcza\n",
      "w \n",
      "kontekście \n",
      "fałszywych \n",
      "informacji \n",
      "i\n",
      "deepfake'ów.\n",
      "Sztuczna inteligencja dedukcyjna:\n",
      "Skoncentrowana na wykorzystywaniu reguł\n",
      "logicznych \n",
      "do \n",
      "wyciągania \n",
      "wniosków \n",
      "i\n",
      "podejmowania decyzji. Ten rodzaj AI działa na\n",
      "podstawie dostarczonych danych i reguł, nie\n",
      "wymaga eksploracyjnego uczenia się.\n",
      "Sztuczna inteligencja indukcyjna: \n",
      "Oparta na zdolności do wyciągania ogólnych\n",
      "zasad lub wniosków na podstawie konkretnych\n",
      "przykładów. Indukcyjna sztuczna inteligencja\n",
      "ma \n",
      "zdolność \n",
      "uczenia \n",
      "się \n",
      "na \n",
      "podstawie\n",
      "doświadczeń i dostarczonych danych.\n",
      "Sztuczna \n",
      "inteligencja \n",
      "uczenia\n",
      "maszynowego (Machine Learning - ML): \n",
      "To \n",
      "obszar \n",
      "AI, \n",
      "który \n",
      "umożliwia \n",
      "systemom\n",
      "komputerowym naukę bez bezpośredniego\n",
      "programowania. \n",
      "Algorytmy \n",
      "uczenia\n",
      "maszynowego \n",
      "pozwalają \n",
      "maszynom\n",
      "samodzielnie dostosowywać się do nowych\n",
      "danych i poprawiać swoje działania w czasie.\n",
      "Sztuczna inteligencja uczenia głębokiego\n",
      "(Deep Learning): \n",
      "Jest to rodzaj uczenia maszynowego, w którym\n",
      "modele (zwane sieciami neuronowymi) składają\n",
      "się z wielu warstw (stąd \"głębokie\") i są w stanie\n",
      "przetwarzać złożone dane, takie jak obrazy czy\n",
      "dźwięki.\n",
      "Sztuczna inteligencja ewolucyjna: \n",
      "Inspirując się procesami ewolucji biologicznej,\n",
      "sztuczna inteligencja ewolucyjna wykorzystuje\n",
      "algorytmy \n",
      "genetyczne \n",
      "do \n",
      "ewoluowania\n",
      "programów \n",
      "komputerowych \n",
      "w \n",
      "kierunku\n",
      "rozwiązania konkretnego problemu.\n",
      "Sztuczna inteligencja probabilistyczna:\n",
      "Obejmuje \n",
      "modelowanie \n",
      "niepewności \n",
      "i\n",
      "prawdopodobieństw w procesie podejmowania\n",
      "decyzji. \n",
      "Wykorzystuje \n",
      "rachunek\n",
      "prawdopodobieństwa \n",
      "w \n",
      "analizie \n",
      "danych \n",
      "i\n",
      "przewidywaniu wyników.\n",
      "Sztuczna inteligencja oparta na regułach:\n",
      "Wykorzystuje zestawy reguł logicznych do\n",
      "podejmowania decyzji. Systemy te są zazwyczaj\n",
      "używane w problemach, gdzie istnieje jasny\n",
      "zestaw reguł i zależności.\n",
      "Sztuczna inteligencja interakcyjna: \n",
      "Skupiona \n",
      "na \n",
      "zdolności \n",
      "do \n",
      "interakcji \n",
      "z\n",
      "użytkownikami w sposób, który przypomina\n",
      "ludzką komunikację. Obejmuje to chatboty,\n",
      "asystentów \n",
      "wirtualnych \n",
      "i \n",
      "systemy\n",
      "rozpoznawania mowy.\n",
      "Te rodzaje sztucznej inteligencji różnią się w\n",
      "swoich \n",
      "podejściach, \n",
      "zastosowaniach \n",
      "i\n",
      "zdolnościach, \n",
      "co \n",
      "umożliwia \n",
      "dostosowanie\n",
      "technologii do różnych problemów i dziedzin.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context2 = search(model, query=query2)[0][0][\"text\"]\n",
    "print(context2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe7ae3",
   "metadata": {},
   "source": [
    "Again, wrong context were parsed. I assume this is caused because of metric_type=\"L2\", which is very bad for high dimensional spaces (in this case 784).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f2dec",
   "metadata": {},
   "source": [
    "## Second try with cosine metric\n",
    "\n",
    "Create similar collection to above but with cosine_similarity metric_type and try search for the same queries again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29ee0d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rag_texts_and_embeddings_cosine', 'rag_texts_and_embeddings']\n",
      "{'collection_name': 'rag_texts_and_embeddings_cosine', 'auto_id': True, 'num_shards': 1, 'description': 'RAG Texts collection', 'fields': [{'field_id': 100, 'name': 'id', 'description': 'Primary id', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'text', 'description': 'Page text', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 4096}}, {'field_id': 102, 'name': 'embedding', 'description': 'Embedded text', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'functions': [], 'aliases': [], 'collection_id': 461835363954033315, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True, 'created_timestamp': 461835861047115779}\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME_COSINE = \"rag_texts_and_embeddings_cosine\"\n",
    "\n",
    "milvus_client.create_collection(collection_name=COLLECTION_NAME_COSINE, schema=schema)\n",
    "\n",
    "index_params_cosine = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params_cosine.add_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_type=\"HNSW\",\n",
    "    metric_type=\"COSINE\",\n",
    "    params={\"M\": 4, \"efConstruction\": 64},  # lower values for speed\n",
    ")\n",
    "\n",
    "\n",
    "milvus_client.create_index(\n",
    "    collection_name=COLLECTION_NAME_COSINE, index_params=index_params_cosine\n",
    ")\n",
    "\n",
    "# checkout our collection\n",
    "print(milvus_client.list_collections())\n",
    "\n",
    "# describe our collection\n",
    "print(milvus_client.describe_collection(COLLECTION_NAME_COSINE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68f526bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_embeddings(file_json, embeddings_json, collection_name=COLLECTION_NAME_COSINE)\n",
    "milvus_client.load_collection(COLLECTION_NAME_COSINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf5639",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6fa4c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nie znaleziono informacji.\n"
     ]
    }
   ],
   "source": [
    "query3 = \"Czym jest Contrastive Language-Image Pretraining?\"\n",
    "response3 = rag(\n",
    "    model, query3, collection_nane=COLLECTION_NAME_COSINE, metric_type=\"COSINE\"\n",
    ")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1b36553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wielkość zasobów internetowych, na podstawie\n",
      "których LLM rozwija swoje możliwości, staje się\n",
      "coraz większa, a nad ich jakością czuwają\n",
      "twórcy modelu. Przełomem w ustanawianiu\n",
      "standardów z zakresu modelowania języka był\n",
      "ChatGPT-2, który pokazał, jak zwiększenie skali\n",
      "modelu może prowadzić do nowych i bardziej\n",
      "zaawansowanych możliwości. Te umiejętności\n",
      "tworzenia tekstu przez GPT-2 przełożyły się na\n",
      "zdolność do wytwarzania długich, spójnych i\n",
      "kontekstowo poprawnych treści, z tworzeniem\n",
      "esejów, opowiadań, a nawet poezji włącznie.\n",
      "Model wykazał też zdolność do rozumienia i\n",
      "rozwiązywania \n",
      "bardziej \n",
      "złożonych \n",
      "zadań\n",
      "językowych, \n",
      "takich \n",
      "jak \n",
      "rozróżnianie\n",
      "dwuznaczności \n",
      "w \n",
      "tekstach. \n",
      "Kolejne \n",
      "etapy\n",
      "rozwoju popularnego czatu doprowadziły do\n",
      "wersji \n",
      "GPT-4, \n",
      "wykraczającej \n",
      "już \n",
      "poza\n",
      "modelowanie językowe i obejmującej multi-\n",
      "media.\n",
      "Jeśli zastanawiasz się, dlaczego ciągle mówimy\n",
      "o \"przewidywaniu jednego, kolejnego słowa\",\n",
      "podczas gdy na przykład czat GPT bez trudu\n",
      "generuje długie teksty, to odpowiedź jest\n",
      "prosta: modele językowe faktycznie generują\n",
      "długie teksty, ale robią to krok po kroku, słowo\n",
      "po słowie. W rzeczywistości, po wygenerowaniu\n",
      "każdego \n",
      "nowego \n",
      "słowa, \n",
      "model \n",
      "ponownie\n",
      "przetwarza cały poprzedni tekst wraz z nowo\n",
      "dodanym \n",
      "fragmentem, \n",
      "aby \n",
      "wygenerować\n",
      "kolejne słowo. W ten sposób powstaje spójna\n",
      "wypowiedź.\n",
      "W rzeczywistości duże modele językowe (LLM)\n",
      "starają się przewidzieć nie tyle konkretne,\n",
      "następne \n",
      "słowo, \n",
      "ile \n",
      "prawdopodobieństwa\n",
      "użycia różnych słów, które mogą kontynuować\n",
      "dany tekst. Dlaczego najlepszym rozwiązaniem\n",
      "nie \n",
      "zawsze \n",
      "jest \n",
      "szukanie \n",
      "\"najbardziej\n",
      "odpowiedniego\" \n",
      "słowa? \n",
      "Rozważmy \n",
      "to \n",
      "na\n",
      "przykładzie prostej gry.\n",
      "Załóżmy, że masz kontynuować tekst: \"44.\n",
      "prezydent USA (i pierwszy Afroamerykanin na\n",
      "tym stanowisku) to Barack...\". Jeśli pomyślałeś,\n",
      "że następnym słowem powinno być \"Obama\" z\n",
      "prawdopodobieństwem 100%, to się mylisz! W\n",
      "oficjalnych \n",
      "dokumentach \n",
      "imię \n",
      "prezydenta\n",
      "często jest podawane w wersji pełnej, razem z\n",
      "drugim imieniem \"Hussein\". Dlatego dobrze\n",
      "wytrenowany LLM przewidziałby \"Obama\" z\n",
      "prawdopodobieństwem \n",
      "około \n",
      "90%, \n",
      "pozo-\n",
      "stawiając 10% dla \"Hussein\".\n",
      "To obrazuje interesujący aspekt LLM – jego\n",
      "twórczy \n",
      "charakter. \n",
      "Podczas \n",
      "generowania\n",
      "każdego następnego słowa, modele wybierają\n",
      "je \"losowo\" bazując na prawdopodobieństwach\n",
      "wyliczonych \n",
      "podczas \n",
      "treningu \n",
      "na \n",
      "dużych\n",
      "zbiorach tekstów. Dzięki temu, te same modele\n",
      "mogą dawać różne odpowiedzi na identyczne\n",
      "zapytania, podobnie jak ludzie. Eksperymenty\n",
      "pokazały, \n",
      "że \n",
      "modele \n",
      "wybierające \n",
      "zawsze\n",
      "\"najbardziej prawdopodobne\" słowo działają\n",
      "gorzej niż te, które wprowadzają element\n",
      "losowości.\n",
      "Język jest strukturą z zasadami i wyjątkami, a\n",
      "słowa w zdaniach są ze sobą powiązane. \n",
      "Ludzie uczą się tych powiązań naturalnie, a\n",
      "dobre modele językowe muszą odwzorować tę\n",
      "zmienność i bogactwo języka, aby precyzyjnie\n",
      "oceniać prawdopodobieństwa słów w zale-\n",
      "żności od kontekstu.\n",
      "W tym miejscu warto zrobić małe odstępstwo,\n",
      "by \n",
      "zastanowić \n",
      "się, \n",
      "co \n",
      "właściwie \n",
      "znaczy\n",
      "stwierdzenie, \n",
      "że \n",
      "\"model \n",
      "potrafi \n",
      "rozwiązać\n",
      "zadanie\"? Proces sprowadza się do tego, że\n",
      "podajemy modelowi tekst w formie zapytania\n",
      "(prompt), \n",
      "a \n",
      "on \n",
      "generuje \n",
      "odpowiednie\n",
      "kontynuacje. \n",
      "Jeśli \n",
      "te \n",
      "generowane \n",
      "treści\n",
      "zgadzają się z naszymi oczekiwaniami, możemy\n",
      "uznać, \n",
      "że \n",
      "model \n",
      "sprostał \n",
      "postawionemu\n",
      "zadaniu.\n",
      "Wprowadzenie do Dużych\n",
      "Modeli Językowych (LLM)\n",
      "2 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contxet3 = search(\n",
    "    model, query3, collection_name=COLLECTION_NAME_COSINE, metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "print(contxet3[0][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19d7c8",
   "metadata": {},
   "source": [
    "Again same issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "98eb7017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Na podstawie dostarczonego kontekstu, dobrym modelem do generowania sztuki, np. do materiałów marketingowych, jest **Pix2Pix**, należący do kategorii generatywnych modeli grafiki komputerowej. W kontekście wspomniano, że sztuczna inteligencja generatywna ma potencjał zastosowania w sztuce, projektowaniu i produkcji treści multimedialnych.\n"
     ]
    }
   ],
   "source": [
    "query4 = (\n",
    "    \"Jaki model jest dobry do generowania sztuki, np. do materiałów marketingowych?\"\n",
    ")\n",
    "response4 = rag(\n",
    "    model, query4, collection_nane=COLLECTION_NAME_COSINE, metric_type=\"COSINE\"\n",
    ")\n",
    "print(response4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245c50e",
   "metadata": {},
   "source": [
    "Again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9beedee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierwszy etap, czyli preprodukcja, to faza planowania. Obejmuje ona:\n",
      "\n",
      "1.  Zdefiniowanie celu, pomysłu, koncepcji.\n",
      "2.  Zaplanowanie budżetu.\n",
      "3.  Podjęcie decyzji nt. metody produkcji, rodzaju wideo lub gatunku filmowego.\n",
      "\n",
      "Na zakończenie tego etapu powinno się dysponować scenariuszem, scenopisem, harmonogramem, zdefiniowanym budżetem i wybranymi narzędziami potrzebnymi do produkcji i postprodukcji.\n"
     ]
    }
   ],
   "source": [
    "query5 = \"O co chodzi w pierwszym etapie, czyli preprodukcji?. Co ona zawiera??\"\n",
    "response5 = rag(\n",
    "    model, query5, collection_nane=COLLECTION_NAME_COSINE, metric_type=\"COSINE\"\n",
    ")\n",
    "print(response5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f36579fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierwszy etap, czyli preprodukcja, to faza planowania. Obejmuje ona:\n",
      "\n",
      "1.  Zdefiniowanie celu, pomysłu, koncepcji.\n",
      "2.  Zaplanowanie budżetu.\n",
      "3.  Podjęcie decyzji dotyczących metody produkcji, rodzaju wideo lub gatunku filmowego.\n",
      "\n",
      "Na zakończenie tego etapu powinno się dysponować scenariuszem, scenopisem, harmonogramem, zdefiniowanym budżetem i wybranymi narzędziami potrzebnymi do produkcji i postprodukcji.\n"
     ]
    }
   ],
   "source": [
    "query5 = \"O co chodzi w pierwszym etapie, czyli preprodukcji?. Co ona zawiera??\"\n",
    "response5 = rag(model, query5)\n",
    "print(response5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778ac76",
   "metadata": {},
   "source": [
    "With very specified question it was finally able to find answer for the question.\n",
    "\n",
    "Overall: RAG is hard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
